{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ProgSnap2 import ProgSnap2Dataset\n",
    "from ProgSnap2 import PS2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning Data\n",
    "We load our data using the ProgSnap2Dataset class. This comes with both a main event table and a LinkTable giving students final exam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semester = 'S19'\n",
    "PATH = \"data/CodeWorkout/\" + semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ProgSnap2Dataset(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the attempt column, since it's calculated incorrectly\n",
    "data.drop_main_table_column('Attempt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = data.get_main_table()\n",
    "student_table = data.load_link_table('Subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fall 2019 Preprocessing\n",
    "\n",
    "There were some differences between F19 and S19:\n",
    "* In F19 there was an additional assignment (between Assignment 4 and 5), which only ~70% of students completed, likely additional optional practice. We will not use this assignment for prediction, since it is abnormal and not in S19. Since it comes in between the two assignments we are using for prediction, we simply remove it.\n",
    "* In F19 the AssignmentIDs were renamed, so we will update their names\n",
    "* In F19 2 ProblemIDs were renamed (though the solutions were unchanged), so we will update their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(main_table[PS2.AssignmentID].unique()))\n",
    "print(len(main_table[PS2.ProblemID].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assignment has no analogue, but we use 500 to put it between the other 2\n",
    "NEW_F19_ASSIGNMENT = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if semester == 'F19':\n",
    "    assignment_map = {\n",
    "        597: 439,\n",
    "        600: 487,\n",
    "        609: 492,\n",
    "        615: 494,\n",
    "        622: NEW_F19_ASSIGNMENT,\n",
    "        631: 502,\n",
    "    }\n",
    "    print(np.mean(main_table[PS2.AssignmentID].isin(assignment_map)))\n",
    "    main_table[PS2.AssignmentID] = main_table[PS2.AssignmentID].map(assignment_map)\n",
    "    \n",
    "    # Two problems were renamed but are equivalent\n",
    "    problem_map = {problem_id: problem_id for problem_id in main_table[PS2.ProblemID].unique()}\n",
    "    problem_map[736] = 45\n",
    "    problem_map[737] = 48\n",
    "    print(np.mean(main_table[PS2.ProblemID].isin(problem_map)))\n",
    "    main_table[PS2.ProblemID] = main_table[PS2.ProblemID].map(problem_map, na_action='ignore')\n",
    "    \n",
    "    # Overwrite the main table so this is the one that's copied\n",
    "    data.set_main_table(main_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main_table[PS2.AssignmentID].unique()))\n",
    "print(len(main_table[PS2.ProblemID].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table[PS2.ProblemID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Students\n",
    "\n",
    "Here we remove studens who did not take the final exam, since we cannot use these for Task 2 (final exam score prediction). While this does somewhat bias the dataset for Task 1, it also ensures a consistent set of training/testing students for both tasks.\n",
    "\n",
    "We can also see that few students are actually removed this way (381 -> 348 for S19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the SubjectIDs where the final grade is non-0\n",
    "# A 0 grade indicates the student did not take the final\n",
    "print(len(student_table.index))\n",
    "subject_ids = set(student_table[student_table['X-Grade'] != 0][PS2.SubjectID].unique())\n",
    "subject_ids = subject_ids.intersection(set(student_table['SubjectID'].unique()))\n",
    "len(subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "We want to confirm that our selected students have a good and well-distributed number of attempts at all the problems in the dataset, and the most problems were well-attempted. The stats and figures below suggest that this is the case: most problems are attempted by ~300/350 students, and most students complete ~40/50 problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we check how many problems each student attempted\n",
    "main_table_filtered = main_table[main_table[PS2.SubjectID].isin(subject_ids)]\n",
    "problems_per_student = main_table_filtered.groupby(by=['SubjectID']).apply(lambda rows: len(rows[PS2.ProblemID].unique()))\n",
    "# 75% of problems were attempted by at least 40 studens, so that's good\n",
    "problems_per_student.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only 3 attempted fewer than 10 problems\n",
    "sum(problems_per_student < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next we see how many students attempted each problem\n",
    "students_per_problem = main_table_filtered.groupby(by=['AssignmentID', 'ProblemID']).apply(lambda rows: len(rows[PS2.SubjectID].unique()))\n",
    "students_per_problem.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next we plot the number of attemptes on each problem (x) / assignment (color)\n",
    "from matplotlib.cm import viridis\n",
    "\n",
    "assignment_ids = list(students_per_problem.keys().map(lambda x: x[0]))\n",
    "assignment_ids = [sorted(assignment_ids).index(x) for x in assignment_ids]\n",
    "colors = [viridis((float(i)-min(assignment_ids))/(max(assignment_ids)-min(assignment_ids))) for i in assignment_ids]\n",
    "\n",
    "# There's a slight drop-off by assignment, but overall they're well-attempted\n",
    "plt.bar(range(0, len(students_per_problem)), students_per_problem, color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Class Label: Identifying Struggling Students\n",
    "\n",
    "In any student modeling task, our goal is to predict if a student will struggle on the next problem. For this dataset, it's not obvious how to define that struggle.\n",
    "\n",
    "We will define struggle as either:\n",
    "1. Never getting a problem correct or \n",
    "2. Taking more attempts at a problem than 75% of students before getting it correct.\n",
    "\n",
    "The code below justifies this decision by showing that most students get the problem correct _eventually_, and most student with more than the 75th percentil of attempts end up with many more attempts than their peers, indicating struggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate data by problem\n",
    "\n",
    "We first get all scored submissions (`Run.Program` events) and aggregate them by SubjectID and ProblemID, counting the number of attempts until a correct submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = main_table_filtered[main_table_filtered[PS2.EventType] == 'Run.Program'].copy()\n",
    "runs['TimeInt'] = pd.to_datetime(runs[PS2.ServerTimestamp]).apply(lambda x: x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_attempts(rows):\n",
    "    scores = rows[PS2.Score]\n",
    "    # If they scored 1, we return the first time they did so\n",
    "    if (scores.max() == 1):\n",
    "        # Argmax returns the first index of the highest score\n",
    "        # Since the array is 0-indexed, we return +1\n",
    "        return rows[PS2.Score].argmax() + 1\n",
    "    return len(rows.index)\n",
    "    \n",
    "\n",
    "scores = runs.groupby([PS2.SubjectID, PS2.AssignmentID, PS2.ProblemID]).apply(get_attempts)\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confirm that this is different than just the count of runs\n",
    "runs.groupby([PS2.SubjectID, PS2.AssignmentID, PS2.ProblemID]).apply(lambda x: len(x.index)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student_attempts = scores.to_frame('Attempts').reset_index()\n",
    "print(student_attempts.shape)\n",
    "student_attempts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Eventual Success\n",
    "\n",
    "Most student get each problem correct eventually, suggesting that the number of attempts is a more meaningful indicator of succeess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_eventually = runs.groupby([PS2.SubjectID, PS2.AssignmentID, PS2.ProblemID])[PS2.Score].apply(lambda x: max(x) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(correct_eventually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_first_try = runs.groupby([PS2.SubjectID, PS2.AssignmentID, PS2.ProblemID])[PS2.Score].apply(lambda x: x.iloc[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(correct_first_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student_scores = student_attempts.merge(correct_eventually.to_frame('CorrectEventually'), on=[PS2.SubjectID, PS2.AssignmentID, PS2.ProblemID])\n",
    "student_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_scores.CorrectEventually.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a cutoff for \"struggling\"\n",
    "We choose the 75th percentile of attempts as the cutoff for struggling, and visualize this to verify that it meaningfully separates the \"tail\" of more struggling students from the main body. The chart below shows this for all 50 problems, and suggests that this is a reasonable (though by no means objectively correct) cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "problem_attempt_75th = student_scores.groupby(PS2.ProblemID).apply(lambda x: x.Attempts.quantile(0.75))\n",
    "problem_attempt_75th.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "problem_ids = list(student_scores.ProblemID.unique())\n",
    "fig, axs = plt.subplots(5, 10)\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        problem_id = problem_ids[i * 10 + j]\n",
    "        attempts = student_scores[student_scores[PS2.ProblemID] == problem_id].Attempts\n",
    "        p75 = problem_attempt_75th[problem_id] + 1\n",
    "        axs[i, j].hist(attempts)\n",
    "        axs[i, j].vlines(p75, 0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = student_scores['ProblemID'].apply(lambda x: problem_attempt_75th[x])\n",
    "student_scores['Label'] = np.logical_and(student_scores['Attempts'] <= cutoffs, student_scores['CorrectEventually'])\n",
    "student_scores['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "# The percentage of struggling problems per student is well-distributed\n",
    "plt.hist(student_scores.groupby(PS2.SubjectID)['Label'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(student_scores['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(student_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Late Assignments\n",
    "\n",
    "Below we confirm that the 5 assignments are well-spaced out, with a consistent ordering accross students.\n",
    "\n",
    "The latter 2 assignments are what is predicted in Task 1 of the data challenge.\n",
    "\n",
    "We divide the data by assignment, rather than by problem, since within a given assignment studens do problems in a variety of orders (see analysis at the end of this document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_times = runs.groupby([PS2.AssignmentID, PS2.ProblemID])['TimeInt'].median()\n",
    "start_time = min(problem_times)\n",
    "problem_times = (problem_times - start_time) / 10**9\n",
    "\n",
    "problem_successes = runs[runs[PS2.Score] == 1].groupby([PS2.AssignmentID, PS2.ProblemID])['TimeInt'].median()\n",
    "problem_successes = (problem_successes - start_time) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_stats = problem_times.to_frame('MedTime').join(problem_successes.to_frame('MedSuccess')).reset_index()\n",
    "problem_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = range(0, len(problem_stats.index))\n",
    "problem_stats.sort_values('MedTime', inplace=True)\n",
    "plt.scatter(problem_stats['MedTime'], y, c='red')\n",
    "# plt.scatter(problem_stats['MedSuccess'], y, c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignments are, thankfully, already in order\n",
    "assignment_stats = runs.groupby(PS2.AssignmentID).apply(lambda x: np.median(x['TimeInt']))\n",
    "assignment_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_assignments = list(assignment_stats.sort_values().index)\n",
    "if (semester == 'F19'):\n",
    "    valid_assignments.remove(NEW_F19_ASSIGNMENT)\n",
    "valid_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "late_assignments = valid_assignments[-2:]\n",
    "late_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_assignments = valid_assignments[:-2]\n",
    "early_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_scores['IsLateProblem'] = student_scores[PS2.AssignmentID].isin(late_assignments)\n",
    "student_scores.sort_values([PS2.SubjectID, PS2.AssignmentID, 'IsLateProblem', PS2.ProblemID], inplace=True)\n",
    "# Remove attempts not in a valid assignment (for F19)\n",
    "student_scores = student_scores[student_scores[PS2.AssignmentID].isin(valid_assignments)]\n",
    "student_scores.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Here we split out data into training/test datasets, as well as eary problems (used to extract features input into the model) and late problems (where struggle will be predicted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by SubjectID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid subjects must have completed at least one early and one late problem\n",
    "ealry_late_subject_ids = student_scores.groupby(PS2.SubjectID)['IsLateProblem'].apply(lambda x: np.mean(x) > 0 and np.mean(x) < 1)\n",
    "# The vast majority of students have\n",
    "np.mean(ealry_late_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instersect these subjectIDs with the ones who completed the final exam\n",
    "valid_subject_ids = subject_ids.intersection(set(ealry_late_subject_ids.index[ealry_late_subject_ids]))\n",
    "valid_subject_ids = list(valid_subject_ids)\n",
    "valid_subject_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(subject_ids))\n",
    "print(len(valid_subject_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(list(valid_subject_ids), test_size=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the train/test split dataframe against the last saved run\n",
    "ids_df = pd.DataFrame({PS2.SubjectID: valid_subject_ids})\n",
    "ids_df['IsTrain'] = ids_df[PS2.SubjectID].isin(train_ids)\n",
    "ids_df\n",
    "\n",
    "path = os.path.join('data', 'Release', semester)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# If saving, uncomment the top line\n",
    "# ids_df.to_csv(os.path.join(path, 'split.csv'), index=False)\n",
    "cached_df = pd.read_csv(os.path.join(path, 'split.csv'))\n",
    "\n",
    "assert(ids_df.equals(cached_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ids))\n",
    "print(len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_scores_train = student_scores[student_scores[PS2.SubjectID].isin(train_ids)]\n",
    "student_scores_test = student_scores[student_scores[PS2.SubjectID].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(student_scores.shape[0])\n",
    "print(student_scores_train.shape[0])\n",
    "print(student_scores_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Split Into early/late datasets\n",
    "In Task 1, we need an early set of problems to use to extract features for the model, and a late set of problems where we're actually predicting student outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_train = student_scores_train[student_scores_train['IsLateProblem'] == False].drop(['IsLateProblem'], axis=1)\n",
    "early_test = student_scores_test[student_scores_test['IsLateProblem'] == False].drop(['IsLateProblem'], axis=1)\n",
    "late_train = student_scores_train[student_scores_train['IsLateProblem']].drop(['IsLateProblem', 'Attempts', 'CorrectEventually'], axis=1)\n",
    "late_test = student_scores_test[student_scores_test['IsLateProblem']].drop(['IsLateProblem', 'Attempts', 'CorrectEventually'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(early_train.shape)\n",
    "early_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(early_test.shape)\n",
    "early_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(late_train.shape)\n",
    "late_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(late_test.shape)\n",
    "late_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join('data', 'Release', semester)\n",
    "os.makedirs(os.path.join(base_path, 'Train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_path, 'Test'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_train.to_csv(os.path.join(base_path, 'Train', 'early.csv'), index=False)\n",
    "late_train.to_csv(os.path.join(base_path, 'Train', 'late.csv'), index=False)\n",
    "early_test.to_csv(os.path.join(base_path, 'Test', 'early.csv'), index=False)\n",
    "late_test.drop('Label', axis=1).to_csv(os.path.join(base_path, 'Test', 'late.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save_subset(os.path.join(base_path, 'Train', 'Data'), lambda df: df[df[PS2.SubjectID].isin(train_ids) & df[PS2.AssignmentID].isin(valid_assignments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save_subset(os.path.join(base_path, 'Test', 'Data'), lambda df: df[df[PS2.SubjectID].isin(test_ids) & df[PS2.AssignmentID].isin(early_assignments)], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the whole S19 Dataset\n",
    "\n",
    "After the practice phase, we don't need the S19 test dataset anymore, so we can release the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if semester == 'S19':\n",
    "    early_all = student_scores[student_scores['IsLateProblem'] == False].drop(['IsLateProblem'], axis=1)\n",
    "    late_all = student_scores[student_scores['IsLateProblem']].drop(['IsLateProblem'], axis=1)\n",
    "\n",
    "    base_path_all = os.path.join('data', 'Release', semester, 'All')\n",
    "    os.makedirs(base_path_all, exist_ok=True)\n",
    "    \n",
    "    early_all.to_csv(os.path.join(base_path_all, 'early.csv'), index=False)\n",
    "    late_all.to_csv(os.path.join(base_path_all, 'late.csv'), index=False)\n",
    "    \n",
    "    data.save_subset(os.path.join(base_path_all, 'Data'), lambda df: df[df[PS2.AssignmentID].isin(valid_assignments)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Predicting Student Grades\n",
    "For Task 2, we are predicting students' Final Exam grades. We just add the appropriate LinkTable _without_ the actual grades, just leaving the SubjectIDs to predict for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_table_dir = os.path.join(base_path, 'Test', 'Data', 'LinkTables')\n",
    "os.makedirs(student_table_dir, exist_ok=True)\n",
    "student_table_test = student_table[student_table[PS2.SubjectID].isin(test_ids)]\n",
    "student_table_test.drop('X-Grade', axis=1).to_csv(os.path.join(student_table_dir, 'Subject.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the solution\n",
    "The solution is, of course, not released, but used to judge submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_path = os.path.join('data', 'Solution', semester, 'task1', 'ref')\n",
    "os.makedirs(sol_path, exist_ok=True)\n",
    "late_test.to_csv(os.path.join(sol_path, 'truth.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_path = os.path.join('data', 'Solution', semester, 'task2', 'ref')\n",
    "os.makedirs(sol_path, exist_ok=True)\n",
    "student_table_test.to_csv(os.path.join(sol_path, 'truth.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unused code for identifying the late problems\n",
    "\n",
    "The code below was used to investigate whether we could predict the last 3 problems of every assignment using the first 7. However, students appear to do the assignment in a variety of orders, making this difficult. Additionally, this would leak future data (e.g. the first 7 problems on Assignment 2 could be used to predict the last 3 problems on Assignment 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_order(rows):\n",
    "    return pd.Series({\n",
    "        'StartEventOrder': min(rows[PS2.Order])\n",
    "        # 'EndEventOrder': max(rows[PS2.Order])\n",
    "    })\n",
    "\n",
    "start_orders = main_table_filtered.groupby([PS2.SubjectID, PS2.AssignmentID, PS2.ProblemID]).apply(get_start_end_order)\n",
    "start_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Unfortunately, the last 3 problems aren't always easy to pick apart\n",
    "print(problem_stats[PS2.AssignmentID].unique())\n",
    "assignment1 = problem_stats[problem_stats[PS2.AssignmentID] == 439]\n",
    "y = range(0, 10)\n",
    "plt.scatter(assignment1['MedTime'], y, c='red')\n",
    "plt.scatter(assignment1['MedSuccess'], y, c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "late_problems = problem_stats.groupby(PS2.AssignmentID).apply(lambda rows: list(rows[PS2.ProblemID][rows['MedTime'].argsort()][-3:]))\n",
    "late_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "late_problem_ids_old = [st for row in late_problems for st in row]\n",
    "late_problem_ids_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runs_time = runs.copy()\n",
    "runs_time['IsLateProblemInAssignment'] = runs_time[PS2.ProblemID].isin(late_problem_ids_old)\n",
    "runs_time.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_late_attempts = runs_time[runs_time['IsLateProblemInAssignment']].groupby([PS2.AssignmentID, PS2.SubjectID])['TimeInt'].min()\n",
    "first_late_attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_late = runs_time.merge(first_late_attempts.to_frame('FirstLateAttempt'), on=[PS2.AssignmentID, PS2.SubjectID], how='left')\n",
    "np.mean(runs_late['FirstLateAttempt'].isna())\n",
    "# ~2% of student-assignments did not have a late attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_late['IsLateAttempt'] = runs_late['TimeInt'] >= runs_late['FirstLateAttempt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(~runs_late['IsLateAttempt'] & runs_late['IsLateProblemInAssignment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30% of attempts at early problems occurred after the first attempt at a late problem\n",
    "np.sum(runs_late['IsLateAttempt'] & ~runs_late['IsLateProblemInAssignment']) / np.sum(~runs_late['IsLateProblemInAssignment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "subjects_sample = np.random.choice(runs_time[PS2.SubjectID].unique(), 20)\n",
    "assignment_sample = runs_time[(runs_time[PS2.AssignmentID] == 439) & runs_time[PS2.SubjectID].isin(subjects_sample)]\n",
    "assignment_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_subject_ids = assignment_sample[PS2.SubjectID]\n",
    "distinct_subject_ids = all_subject_ids.unique()\n",
    "subject_indices = [sorted(distinct_subject_ids).index(x) for x in all_subject_ids]\n",
    "colors = [viridis(float(i)) for i in assignment_sample['IsLateProblemInAssignment']]\n",
    "subject_times_norm = assignment_sample.groupby('SubjectID')['TimeInt'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "widths = list(assignment_sample[PS2.Score].apply(lambda x: 0.2 if x < 1 else 3))\n",
    "plt.scatter(x=subject_times_norm, y=subject_indices, color=colors, linewidths=widths, edgecolors=None)\n",
    "plt.xlim([-2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_sorted = lambda a: np.all(a[:-1] <= a[1:])\n",
    "\n",
    "def is_consistent(rows):\n",
    "    orders = rows[PS2.Order]\n",
    "    times = rows['TimeInt']\n",
    "    return is_sorted(times.values)\n",
    "\n",
    "consistent = runs.groupby([PS2.SubjectID, PS2.AssignmentID, PS2.ProblemID]).apply(is_consistent)\n",
    "consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runs[(runs[PS2.ProblemID]==102) & (runs[PS2.SubjectID]=='71ffa17407d66e134442eebb32d330ec')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consistent[~consistent]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
